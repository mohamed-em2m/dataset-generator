{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-26T04:41:10.702138Z","iopub.execute_input":"2023-08-26T04:41:10.702430Z","iopub.status.idle":"2023-08-26T04:41:10.714018Z","shell.execute_reply.started":"2023-08-26T04:41:10.702404Z","shell.execute_reply":"2023-08-26T04:41:10.713079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.set_default_device('cuda')","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:41:10.715686Z","iopub.execute_input":"2023-08-26T04:41:10.716260Z","iopub.status.idle":"2023-08-26T04:41:15.089084Z","shell.execute_reply.started":"2023-08-26T04:41:10.716227Z","shell.execute_reply":"2023-08-26T04:41:15.087972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport pandas as pd\n\n\n# Load QuaC dataset from JSON\ndef load(file):\n        with open(file, \"r\", encoding=\"utf-8\") as json_file:\n            quac_data = json.load(json_file)\n\n        # Initialize lists to store data\n        context_list = []\n        question_list = []\n        answer_list = []\n\n        # Extract data from JSON and populate lists\n        for dialog in quac_data[\"data\"]:\n            for paragraph in dialog[\"paragraphs\"]:\n                context = paragraph[\"context\"]\n                for qa in paragraph[\"qas\"]:\n                    question = qa[\"question\"]\n                    answer = qa[\"orig_answer\"][\"text\"]\n                    context_list.append(context)\n                    question_list.append(question)\n                    answer_list.append(answer)\n\n        # Create a DataFrame from the lists\n        data = {\"context\": context_list, \"question\": question_list, \"answer\": answer_list}\n        df = pd.DataFrame(data)\n\n        # Save DataFrame to CSV\n\n        print(\"Conversion completed.\")\n        return df","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:41:15.094726Z","iopub.execute_input":"2023-08-26T04:41:15.097455Z","iopub.status.idle":"2023-08-26T04:41:15.108108Z","shell.execute_reply.started":"2023-08-26T04:41:15.097416Z","shell.execute_reply":"2023-08-26T04:41:15.106823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=load(\"/kaggle/input/quac-question-answering-in-context-dataset/train_v0.2 QuaC.json\")","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:41:15.109497Z","iopub.execute_input":"2023-08-26T04:41:15.110030Z","iopub.status.idle":"2023-08-26T04:41:17.452893Z","shell.execute_reply.started":"2023-08-26T04:41:15.109998Z","shell.execute_reply":"2023-08-26T04:41:17.451880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid=load('/kaggle/input/quac-question-answering-in-context-dataset/val_v0.2 QuaC.json')","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:41:17.456382Z","iopub.execute_input":"2023-08-26T04:41:17.456673Z","iopub.status.idle":"2023-08-26T04:41:17.670492Z","shell.execute_reply.started":"2023-08-26T04:41:17.456647Z","shell.execute_reply":"2023-08-26T04:41:17.668706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['queans']=train['question']+'# '+train['answer']\ntrain['queans']=train['queans'].apply(lambda x:str(x).lower())","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:41:17.671879Z","iopub.execute_input":"2023-08-26T04:41:17.672338Z","iopub.status.idle":"2023-08-26T04:41:17.752852Z","shell.execute_reply.started":"2023-08-26T04:41:17.672302Z","shell.execute_reply":"2023-08-26T04:41:17.751896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid['queans']=valid['question']+'# '+valid['answer']\nvalid['queans']=valid['queans'].apply(lambda x:str(x).lower())","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:41:17.754389Z","iopub.execute_input":"2023-08-26T04:41:17.754747Z","iopub.status.idle":"2023-08-26T04:41:17.767616Z","shell.execute_reply.started":"2023-08-26T04:41:17.754713Z","shell.execute_reply":"2023-08-26T04:41:17.766599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:41:17.769303Z","iopub.execute_input":"2023-08-26T04:41:17.770008Z","iopub.status.idle":"2023-08-26T04:41:17.792224Z","shell.execute_reply.started":"2023-08-26T04:41:17.769975Z","shell.execute_reply":"2023-08-26T04:41:17.791213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"mohamedemam/QA_GeneraToR\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"mohamedemam/QA_GeneraToR\")","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:41:17.793802Z","iopub.execute_input":"2023-08-26T04:41:17.794138Z","iopub.status.idle":"2023-08-26T04:42:03.420079Z","shell.execute_reply.started":"2023-08-26T04:41:17.794107Z","shell.execute_reply":"2023-08-26T04:42:03.419072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:42:03.421589Z","iopub.execute_input":"2023-08-26T04:42:03.422152Z","iopub.status.idle":"2023-08-26T04:42:03.427225Z","shell.execute_reply.started":"2023-08-26T04:42:03.422115Z","shell.execute_reply":"2023-08-26T04:42:03.426094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:42:03.432609Z","iopub.execute_input":"2023-08-26T04:42:03.432981Z","iopub.status.idle":"2023-08-26T04:42:03.440047Z","shell.execute_reply.started":"2023-08-26T04:42:03.432946Z","shell.execute_reply":"2023-08-26T04:42:03.439012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question_words = [\n\"which\", \"how\", \"when\", \"where\", \"who\", \"whom\", \"whose\", \"why\",\n    \"which\", \"who\", \"whom\", \"whose\",    \"whereas\"\n   \n]\nst= [   \"can\", \"could\", \"may\", \"might\", \"will\", \"would\", \"shall\", \"should\",\n    \"do\", \"does\", \"did\", \"is\", \"are\", \"am\", \"was\", \"were\", \"be\", \"being\", \"been\",\n    \"have\", \"has\", \"had\",\"if\"  ,  \"is\", \"are\", \"am\", \"was\", \"were\", \"do\", \"does\", \"did\", \"can\", \"could\",\n    \"will\", \"would\", \"shall\", \"should\", \"might\", \"may\", \"must\",\n    \"may\", \"might\", \"must\",\n\n]","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:42:03.441321Z","iopub.execute_input":"2023-08-26T04:42:03.441881Z","iopub.status.idle":"2023-08-26T04:42:03.453478Z","shell.execute_reply.started":"2023-08-26T04:42:03.441848Z","shell.execute_reply":"2023-08-26T04:42:03.452513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef change(x):\n    x=re.split(r'\\s+|\\'',x)\n    if x[0] in st:\n        return x[0]\n    for i in x:\n        if i in question_words:\n            return i\n    if \"what\" in x:\n         w=x.index(\"what\")\n         if ' '.join(x[max(w-1,0):min(w+1,len(x)-1)]) in [\"in what\",'for what','on what','to what'] :\n            return ' '.join(x[w-1:w+1]) \n         if  x[min(w+1,len(x)-1)] in st:\n            return ' '.join(x[w:min(w+2,len(x)-1)])\n         return  \"what\"\n        \n    return \" \"    ","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:42:03.454674Z","iopub.execute_input":"2023-08-26T04:42:03.455133Z","iopub.status.idle":"2023-08-26T04:42:03.465614Z","shell.execute_reply.started":"2023-08-26T04:42:03.455100Z","shell.execute_reply":"2023-08-26T04:42:03.464642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['start']=train['question'].apply(lambda x: change(x.lower()))\nvalid['start']=valid['question'].apply(lambda x: change(x.lower()))","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:42:03.466739Z","iopub.execute_input":"2023-08-26T04:42:03.467576Z","iopub.status.idle":"2023-08-26T04:42:04.166612Z","shell.execute_reply.started":"2023-08-26T04:42:03.467544Z","shell.execute_reply":"2023-08-26T04:42:04.165609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['context']=train['start']+\": \"+train['context']\nvalid['context']=valid['start']+\": \"+valid['context']","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:42:04.168038Z","iopub.execute_input":"2023-08-26T04:42:04.168402Z","iopub.status.idle":"2023-08-26T04:42:04.253312Z","shell.execute_reply.started":"2023-08-26T04:42:04.168368Z","shell.execute_reply":"2023-08-26T04:42:04.252365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['start'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:42:04.254869Z","iopub.execute_input":"2023-08-26T04:42:04.255240Z","iopub.status.idle":"2023-08-26T04:42:04.284891Z","shell.execute_reply.started":"2023-08-26T04:42:04.255206Z","shell.execute_reply":"2023-08-26T04:42:04.283814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nimport torch\nclass TextClassificationDataset(Dataset):\n    def __init__(self,inputs,labels ):\n               self.labels=labels\n               self.inputs=inputs\n                \n    def __len__(self):\n        return len(self.inputs)\n    def __getitem__(self, idx):\n        a=tokenizer(self.inputs.iloc[idx],max_length=512,truncation=True,padding=\"max_length\",return_tensors='pt')\n        b=tokenizer(self.labels.iloc[idx],max_length=40,truncation=True ,padding=\"max_length\",return_tensors='pt')\n\n        return {\n            \"input_ids\": a[\"input_ids\"] [0],\n            \"attention_mask\": a[\"attention_mask\"][0],\n            \"labels\":b['input_ids'][0],\n        }","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:42:04.286540Z","iopub.execute_input":"2023-08-26T04:42:04.287145Z","iopub.status.idle":"2023-08-26T04:42:04.301642Z","shell.execute_reply.started":"2023-08-26T04:42:04.287111Z","shell.execute_reply":"2023-08-26T04:42:04.300734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train2=TextClassificationDataset(train['context'],train['queans'])\nvalid2=TextClassificationDataset(valid['context'],valid['queans'])","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:42:04.302977Z","iopub.execute_input":"2023-08-26T04:42:04.303762Z","iopub.status.idle":"2023-08-26T04:42:04.313424Z","shell.execute_reply.started":"2023-08-26T04:42:04.303728Z","shell.execute_reply":"2023-08-26T04:42:04.312494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train2, batch_size=1, shuffle=True,generator=torch.Generator('cuda') )\nvalid_dataloader=DataLoader(valid2,batch_size=1, shuffle=True,generator=torch.Generator('cuda'))","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:42:04.314636Z","iopub.execute_input":"2023-08-26T04:42:04.315197Z","iopub.status.idle":"2023-08-26T04:42:04.325408Z","shell.execute_reply.started":"2023-08-26T04:42:04.315142Z","shell.execute_reply":"2023-08-26T04:42:04.324498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nl_train=[]\nv_train=[]","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:42:04.326803Z","iopub.execute_input":"2023-08-26T04:42:04.327332Z","iopub.status.idle":"2023-08-26T04:42:04.337201Z","shell.execute_reply.started":"2023-08-26T04:42:04.327274Z","shell.execute_reply":"2023-08-26T04:42:04.336078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.cuda.amp import GradScaler, autocast\nscaler = GradScaler()","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:42:04.338691Z","iopub.execute_input":"2023-08-26T04:42:04.339342Z","iopub.status.idle":"2023-08-26T04:42:04.349463Z","shell.execute_reply.started":"2023-08-26T04:42:04.339308Z","shell.execute_reply":"2023-08-26T04:42:04.348423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install peft==0.4.0","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:42:04.350921Z","iopub.execute_input":"2023-08-26T04:42:04.351558Z","iopub.status.idle":"2023-08-26T04:42:16.973012Z","shell.execute_reply.started":"2023-08-26T04:42:04.351525Z","shell.execute_reply":"2023-08-26T04:42:16.971868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nfrom datasets import load_dataset\nfrom peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig\nimport torch\nimport numpy as np\n# Load the BERT model\n\n# Create a LoRA configuration\nconfig = LoraConfig(r=10)\n\n# Wrap the BERT model with LoRA\nlora_model = PeftModel(model, config)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:42:16.974844Z","iopub.execute_input":"2023-08-26T04:42:16.975805Z","iopub.status.idle":"2023-08-26T04:42:16.985678Z","shell.execute_reply.started":"2023-08-26T04:42:16.975772Z","shell.execute_reply":"2023-08-26T04:42:16.983752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"model = get_peft_model(model, config)\nmodel.print_trainable_parameters()\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:42:16.989766Z","iopub.execute_input":"2023-08-26T04:42:16.990338Z","iopub.status.idle":"2023-08-26T04:42:17.004501Z","shell.execute_reply.started":"2023-08-26T04:42:16.990301Z","shell.execute_reply":"2023-08-26T04:42:17.003457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=torch.load(\"/kaggle/input/qa-generate/model1.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:42:17.006115Z","iopub.execute_input":"2023-08-26T04:42:17.006531Z","iopub.status.idle":"2023-08-26T04:42:45.576062Z","shell.execute_reply.started":"2023-08-26T04:42:17.006499Z","shell.execute_reply":"2023-08-26T04:42:45.575037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from huggingface_hub import notebook_login\n\n#notebook_login(\"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\")","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:42:45.577804Z","iopub.execute_input":"2023-08-26T04:42:45.578173Z","iopub.status.idle":"2023-08-26T04:42:45.611506Z","shell.execute_reply.started":"2023-08-26T04:42:45.578139Z","shell.execute_reply":"2023-08-26T04:42:45.610594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model.push_to_hub(\"mohamedemam/QA_GeneraToR\")","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:42:56.902263Z","iopub.execute_input":"2023-08-26T04:42:56.902645Z","iopub.status.idle":"2023-08-26T04:44:54.967615Z","shell.execute_reply.started":"2023-08-26T04:42:56.902613Z","shell.execute_reply":"2023-08-26T04:44:54.965538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import  AdamW,Adafactor\n\noptimizer = Adafactor(\n    model.parameters(),\n    lr=1e-4,\n    relative_step=False,\n    scale_parameter=False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:42:48.074413Z","iopub.status.idle":"2023-08-26T04:42:48.074889Z","shell.execute_reply.started":"2023-08-26T04:42:48.074634Z","shell.execute_reply":"2023-08-26T04:42:48.074655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:42:48.076474Z","iopub.status.idle":"2023-08-26T04:42:48.077475Z","shell.execute_reply.started":"2023-08-26T04:42:48.077218Z","shell.execute_reply":"2023-08-26T04:42:48.077247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs=3\n# Step 4: Fine-Tuning Loop\nfor epoch in range(1):\n    torch.save(model,f\"model.pth\")\n\n    model.train()\n    total_loss = 0\n    loop=tqdm(train_dataloader,leave=True)\n    o=0\n    for batch in loop:\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        labels = batch[\"labels\"]\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n       # loss.backward()\n    \n        scaler.scale(loss).backward()\n\n        if ((o+1)%32==0):\n\n        # Unscales gradients and updates the model's parameters\n                scaler.step(optimizer)\n                # Updates the scale for next iteration\n                scaler.update()\n                optimizer.zero_grad()\n\n        total_loss += loss.item()\n\n        loop.set_description(f'Epoch {o}')\n        loop.set_postfix(loss=loss.item())\n        o+=1\n\n    avg_loss = total_loss / len(train_dataloader)\n    print(f\"Epoch {epoch+1} - Average Loss: {avg_loss}\")\n    # ... (previous code)\n\n# Step 4: Fine-Tuning Loop with Validation\n\n    # Validation\n    model.eval()\n    total_val_loss = 0\n\n    with torch.no_grad():\n        loop=tqdm(valid_dataloader,leave=True)\n\n        for batch in loop:\n            input_ids = batch[\"input_ids\"]\n            attention_mask = batch[\"attention_mask\"]\n            labels = batch[\"labels\"]\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            total_val_loss += loss.item()\n            loop.set_description(f'Epoch {o}')\n            loop.set_postfix(loss=loss.item())\n\n    torch.save(model,f\"model1.pth\")\n    avg_val_loss = total_val_loss / len(valid_dataloader)\n    print(f\"Epoch {epoch+1} - Validation Loss: {avg_val_loss}\")\n    l_train.append(avg_loss)\n    v_train.append(avg_val_loss)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:42:48.078988Z","iopub.status.idle":"2023-08-26T04:42:48.079466Z","shell.execute_reply.started":"2023-08-26T04:42:48.079228Z","shell.execute_reply":"2023-08-26T04:42:48.079251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()","metadata":{"execution":{"iopub.status.busy":"2023-08-26T04:44:54.969773Z","iopub.execute_input":"2023-08-26T04:44:54.970129Z","iopub.status.idle":"2023-08-26T04:44:54.994916Z","shell.execute_reply.started":"2023-08-26T04:44:54.970094Z","shell.execute_reply":"2023-08-26T04:44:54.993848Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"text=\"\"\"1 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state htâˆ’1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc\u0002tion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence\u0002aligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence\u0002aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9]\"\"\"\nz=tokenizer(text,max_length=512,truncation=True,padding=\"max_length\",return_tensors='pt')","metadata":{"execution":{"iopub.status.busy":"2023-08-26T05:07:00.146482Z","iopub.execute_input":"2023-08-26T05:07:00.146912Z","iopub.status.idle":"2023-08-26T05:07:00.161249Z","shell.execute_reply.started":"2023-08-26T05:07:00.146876Z","shell.execute_reply":"2023-08-26T05:07:00.160238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.batch_decode(model.generate(**z,temperature=2.4 ,top_p=.1,length_penalty=7,num_return_sequences=6,do_sample=True),skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T05:07:00.658056Z","iopub.execute_input":"2023-08-26T05:07:00.659124Z","iopub.status.idle":"2023-08-26T05:07:01.749876Z","shell.execute_reply.started":"2023-08-26T05:07:00.659081Z","shell.execute_reply":"2023-08-26T05:07:01.748156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train['answer'].str.strip()=='cannotanswer']","metadata":{"execution":{"iopub.status.busy":"2023-08-26T05:00:54.036762Z","iopub.execute_input":"2023-08-26T05:00:54.037149Z","iopub.status.idle":"2023-08-26T05:00:54.108824Z","shell.execute_reply.started":"2023-08-26T05:00:54.037114Z","shell.execute_reply":"2023-08-26T05:00:54.107861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}